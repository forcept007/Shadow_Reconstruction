# Shadow_Reconstruction

Recent advancements have shown that deep learning is an extremely powerful technology. It offers exciting opportunities and spectacular applications. However, the ethical and responsible use of deep learning requires that we become fully aware of the associated risks. One such risk is privacy: trained models potentially store privacy-sensitive information about the training data in their parameters. Without additional measures, this leaves some models vulnerable to attacks designed to infer sensitive information, or to even reconstruct the original training samples.

Model inversion attacks are attempts to reconstruct an input sample from the parameters (‘weights’) of a neural network trained on a dataset containing this sample. It has been shown that one can make recognizable reconstructions if the value of each output node of the network for a given target sample is known (Fredrikson et al., 2015), or when making use of recent theoretical advancements describing the relationship between datasets and parameters of neural networks (Haim et al., 2022, Buzaglo et al., 2023). However, in these cases, one needs to have access to the original neural network model. In this challenge, the goal is to obtain good reconstructions without knowledge of the original network parameters.
