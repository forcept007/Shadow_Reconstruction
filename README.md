# Shadow Reconstructions

Model inversion (MI) attacks are categorized into optimization approaches, which use gradient-based optimization and inversion via a secondary model. This study bridges the gap between these two methods, focusing on a gray-box setting where the model's architecture is known but not its weights. The focus is on using a transposed convolutional neural network (TCNN) as an inverse model to reconstruct images from a convolutional neural network's (CNN) output vectors. To address the gray-box setting, a shadow model is trained on the MNIST dataset to mimic the target model's behavior. The effect on image quality is tested based on the input combinations for the TCNN, which consist of the output vector, gradient reconstructions obtained from the gradient-based optimization approach, and activations from the shadow model's last linear layer. Results on the MNIST dataset show that the stand-alone gradient-based optimization reconstructions have an average SSIM of 0.2409 $\pm$ 0.0068 and an average MSE of 0.0672 $\pm$ 0.0021. Furthermore, the TCNN, acting as the inverse model of the target model, achieves an average SSIM of 0.2298 $\pm$ 0.0041 and an average MSE of 0.0966 $\pm$ 0.0011 when passing just the output vector through the model. The results also showed that from the different input combinations, using gradient reconstructions as the input for the TCNN significantly enhances image fidelity, achieving an average SSIM value of 0.3727 $\pm$ 0.0030 and an average MSE value of 0.0649 $\pm$ 0.0009. These reconstructions have a 54.73\% higher SSIM value than the stand-alone gradient-based optimization reconstructions. The reconstructions have 62.19\% higher SSIM values than the standard TCNN reconstructions, which only obtain the output vector as input. These findings highlight the benefit of utilizing both directions of MI attacks to obtain the highest quality reconstructions. The effects of defensive techniques like output vector rounding and truncation are also examined. Rounding has minimal impact, while truncation increases SSIM; it makes the generated images converge to the class average and lose the subtle features of a given target image. This study highlights the risks of data leakage in deep learning and emphasizes the need for robust defense mechanisms to protect sensitive information against MI attacks.


## Research Question:
How do different factors influence the fidelity of image reconstruction using TCNNs within a non-white box framework? Specifically:

RQ1: How do variations in input combinations to the TCNN, including output vector, gradient reconstruction, and last linear layer activations, impact reconstruction quality?    

RQ2: How do defensive mechanisms, like truncation and output vector rounding, affect the quality of the reconstructed images?

## Findings:
The concept of MI attacks was the focus of this research. MI attacks have branched into two directions. The first is the optimization approach, which inverts a model using gradient-based optimization in the data space. The second direction is the training of the second model, which acts as the inverse of the original model. This work combined both approaches and tested them in a non-white box framework, which entails no direct access to the target model parameters and weights. The main research question was: ``How do different factors influence the fidelity of image reconstruction using transposed CNNs within a non-white box framework?" 

The research incorporated shadow models, which played a critical role in the reconstruction process. These shadow models were essential in a gray-box setting, where the attacker's knowledge is limited to the target model's architecture but not its weights. Training a shadow model on a disjoint dataset from the same distribution as the target model's training data made it possible to mimic the target model's behavior. This approach allowed for effective reconstructions without access to the target model's internal parameters.

To address the main research question, the first sub-question (RQ1) explored how different input combinations to the TCNN—namely the output vector, gradient reconstruction, and activations from the last linear layer—affect reconstruction quality. The study demonstrated that incorporating gradient reconstructions improves the fidelity of image reconstructions. The TCNN model that used only the gradient reconstruction (denoted as $M_{I}(\tilde{X})$) achieved the highest SSIM average of $0.3727 \pm 0.0030$ and an MSE average of $0.0649 \pm 0.0009$ on the test set $D_F$.

The study also showed that combining the TCNN architecture with the gradient-based reconstructions proved superior to the standalone gradient-based method on the test set, with an average SSIM increase of $54.73\%$. This significant improvement underscores the TCNN's ability to preserve spatial details better than a gradient-based optimization approach. The improvement also highlights the benefit of combining both approaches of MI attacks outlined above.

To address the main research question further, the second sub-question (RQ2) examined how defensive mechanisms, such as truncation and output vector rounding, impact the quality of reconstructed images. Output vector rounding to one decimal place showed no significant deterioration in reconstruction quality, indicating the robustness of the TCNN model to minor perturbations in the input. Truncation of the output vector to the top-$k$ scores (with $k=10, 5, 3, 1$) revealed that while SSIM values increased with fewer dimensions, this came at the cost of losing finer details in the reconstructions, leading to more generalized images.

It is essential to acknowledge the limitations of this study. The reliance on the MNIST dataset \cite{lecun2010mnist}, while necessary for the initial exploration, does limit the generalizability of the findings to more complex data. Additionally, the study primarily focused on relatively simple model architectures, which may only partially represent the diversity of real-world models. More complex models, such as deeper networks or those with different structures (e.g., ResNet, EfficientNet), might respond differently to the input combinations and defensive mechanisms tested. Another limitation of the work is the assumption that the attacker has access to a dataset from the same distribution as the target dataset, as this assumption may not always hold in a real-world setting. These considerations are crucial for comprehensively understanding the research's scope and implications.

# Code

Run _bep_final_code_notebook.ipynb_ file to obtain all required results found in the powerpoint presentation and the research paper
